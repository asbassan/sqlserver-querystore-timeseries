{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542ccb1c",
   "metadata": {},
   "source": [
    "# Model Benchmarking RMSE Summary Table (all queries/metrics)\n",
    "\n",
    "This notebook generates summary RMSEs for all QueryName and Metric combinations in SimulatedQueryMetrics.csv, for ARIMA, Prophet, LSTM, Random Forest, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5846fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas scikit-learn xgboost statsmodels prophet tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebc8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06bd64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'SimulatedQueryMetrics.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df['MetricDate'] = pd.to_datetime(df['MetricDate'])\n",
    "metrics = ['CPU', 'LatencyMs', 'LogicalReads']\n",
    "queries = df['QueryName'].unique()\n",
    "lags = 7\n",
    "\n",
    "def create_lag_features(df, lags=7, val_col='y'):\n",
    "    df = df.copy()\n",
    "    for lag in range(1, lags+1):\n",
    "        df[f'lag_{lag}'] = df[val_col].shift(lag)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d81c7",
   "metadata": {},
   "source": [
    "## Run Benchmarking for All Query-Metric Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f2a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:16:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:16:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:16:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:16:58 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:17:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:17:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D265545760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:17:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:17:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D2631F79C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:17:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:17:34 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:17:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:17:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    }
   ],
   "source": [
    "summary_rows = []\n",
    "for query in queries:\n",
    "    for metric in metrics:\n",
    "        dfq = df[df['QueryName'] == query].copy().sort_values('MetricDate').reset_index(drop=True)\n",
    "        if dfq[metric].isnull().all() or len(dfq) < lags + 10:\n",
    "            continue  # skip if not enough data\n",
    "        # Data prep\n",
    "        dfq_sub = dfq[['MetricDate', metric, 'QueryVariant']].rename(columns={'MetricDate':'ds', metric:'y'})\n",
    "        dfq_lagged = create_lag_features(dfq_sub, lags=lags, val_col='y')\n",
    "        split = int(len(dfq_lagged) * 0.8)\n",
    "        train_df = dfq_lagged.iloc[:split]\n",
    "        test_df = dfq_lagged.iloc[split:]\n",
    "        X_train = train_df[[f'lag_{i}' for i in range(1, lags+1)]]\n",
    "        y_train = train_df['y']\n",
    "        X_test = test_df[[f'lag_{i}' for i in range(1, lags+1)]]\n",
    "        y_test = test_df['y']\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            arima_train = train_df['y']\n",
    "            arima_test = test_df['y']\n",
    "            arima_model = ARIMA(arima_train, order=(lags, 0, 0))\n",
    "            arima_fit = arima_model.fit()\n",
    "            arima_pred = arima_fit.forecast(steps=len(arima_test))\n",
    "            arima_rmse = mean_squared_error(arima_test, arima_pred, squared=False)\n",
    "        except Exception as e:\n",
    "            arima_rmse = np.nan\n",
    "\n",
    "        # Prophet\n",
    "        try:\n",
    "            prophet_train = train_df[['ds', 'y']]\n",
    "            prophet_test = test_df[['ds', 'y']]\n",
    "            m = Prophet()\n",
    "            m.fit(prophet_train)\n",
    "            future = prophet_test[['ds']]\n",
    "            forecast = m.predict(future)\n",
    "            prophet_rmse = mean_squared_error(prophet_test['y'], forecast['yhat'], squared=False)\n",
    "        except Exception as e:\n",
    "            prophet_rmse = np.nan\n",
    "\n",
    "        # LSTM\n",
    "        try:\n",
    "            X_train_lstm = X_train.values.reshape((-1, lags, 1))\n",
    "            X_test_lstm = X_test.values.reshape((-1, lags, 1))\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(32, input_shape=(lags, 1)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "            lstm_pred = model.predict(X_test_lstm).flatten()\n",
    "            lstm_rmse = mean_squared_error(y_test, lstm_pred, squared=False)\n",
    "        except Exception as e:\n",
    "            lstm_rmse = np.nan\n",
    "\n",
    "        # Random Forest\n",
    "        try:\n",
    "            rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            rf_pred = rf.predict(X_test)\n",
    "            rf_rmse = mean_squared_error(y_test, rf_pred, squared=False)\n",
    "        except Exception as e:\n",
    "            rf_rmse = np.nan\n",
    "\n",
    "        # XGBoost\n",
    "        try:\n",
    "            xgb = XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "            xgb.fit(X_train, y_train)\n",
    "            xgb_pred = xgb.predict(X_test)\n",
    "            xgb_rmse = mean_squared_error(y_test, xgb_pred, squared=False)\n",
    "        except Exception as e:\n",
    "            xgb_rmse = np.nan\n",
    "\n",
    "        summary_rows.append({\n",
    "            'Query': query,\n",
    "            'Metric': metric,\n",
    "            'ARIMA': np.round(arima_rmse, 2),\n",
    "            'Prophet': np.round(prophet_rmse, 2),\n",
    "            'LSTM': np.round(lstm_rmse, 2),\n",
    "            'Random Forest': np.round(rf_rmse, 2),\n",
    "            'XGBoost': np.round(xgb_rmse, 2)\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145119dd",
   "metadata": {},
   "source": [
    "## Display Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17a2466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Metric</th>\n",
       "      <th>ARIMA</th>\n",
       "      <th>Prophet</th>\n",
       "      <th>LSTM</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>CPU</td>\n",
       "      <td>12.07</td>\n",
       "      <td>13.30</td>\n",
       "      <td>37.72</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>LatencyMs</td>\n",
       "      <td>23.31</td>\n",
       "      <td>37.78</td>\n",
       "      <td>181.08</td>\n",
       "      <td>12.48</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>LogicalReads</td>\n",
       "      <td>20.70</td>\n",
       "      <td>19.28</td>\n",
       "      <td>153.17</td>\n",
       "      <td>14.76</td>\n",
       "      <td>16.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q2</td>\n",
       "      <td>CPU</td>\n",
       "      <td>11.99</td>\n",
       "      <td>10.89</td>\n",
       "      <td>43.59</td>\n",
       "      <td>8.47</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2</td>\n",
       "      <td>LatencyMs</td>\n",
       "      <td>25.22</td>\n",
       "      <td>35.18</td>\n",
       "      <td>187.25</td>\n",
       "      <td>12.84</td>\n",
       "      <td>14.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q2</td>\n",
       "      <td>LogicalReads</td>\n",
       "      <td>21.46</td>\n",
       "      <td>19.96</td>\n",
       "      <td>158.12</td>\n",
       "      <td>15.48</td>\n",
       "      <td>16.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Query        Metric  ARIMA  Prophet    LSTM  Random Forest  XGBoost\n",
       "0    Q1           CPU  12.07    13.30   37.72           8.38     9.40\n",
       "1    Q1     LatencyMs  23.31    37.78  181.08          12.48    14.98\n",
       "2    Q1  LogicalReads  20.70    19.28  153.17          14.76    16.55\n",
       "3    Q2           CPU  11.99    10.89   43.59           8.47     9.09\n",
       "4    Q2     LatencyMs  25.22    35.18  187.25          12.84    14.89\n",
       "5    Q2  LogicalReads  21.46    19.96  158.12          15.48    16.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(summary_rows)\n",
    "# For display: arrange columns like the sample\n",
    "summary_df = summary_df[['Query', 'Metric', 'ARIMA', 'Prophet', 'LSTM', 'Random Forest', 'XGBoost']]\n",
    "display(summary_df)\n",
    "summary_df.to_csv('model_benchmark_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a0769-c979-45a9-8dbc-0462164ae455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
